{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "DIR_PREFIX = \"..\"\n",
    "\n",
    "sys.path.insert(0, DIR_PREFIX)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CommitChronicle_preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#import custom scripts\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCommitChronicle_preprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDatasetParser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetParser\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CommitChronicle_preprocessing'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from peft import PeftModel, PeftConfig\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import wandb\n",
    "from transformers import TrainerCallback\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import PrefixTuningConfig\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "\n",
    "#import custom scripts\n",
    "from CommitChronicle_preprocessing.DatasetParser import DatasetParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "env: WANDB_PROJECT=commit_message_generation\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $(cat $DIR_PREFIX\"/tokens/hf.token\")\n",
    "!huggingface-cli login --token $(cat $DIR_PREFIX\"/tokens/hf_write_token.txt\")\n",
    "!wandb login --relogin $(cat $DIR_PREFIX\"/tokens/wandb_token.txt\")\n",
    "%env WANDB_PROJECT=commit_message_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"Salesforce/codet5p-220m\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32108"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint, device_map=\"auto\", local_files_only=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "parser = DatasetParser(tokenizer)\n",
    "parser.add_special_tokens(tokenizer, model)\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e5b33c901b4da98845811460589435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbee84e00d8d4afabf5e5204f91ac0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(DIR_PREFIX + \"/tokenized_CommitChronicle\")\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.map(parser.add_tokens_to_msg, num_proc=16)\n",
    "train_data = train_data.map(parser.add_tokens_to_msg, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_msgs(example):\n",
    "    example[\"labels\"] = tokenizer(example[\"message\"], return_tensors=\"pt\",\n",
    "                                padding='max_length', truncation=True)['input_ids']\n",
    "    example[\"labels\"][example['labels'] == tokenizer.pad_token_id] = -100\n",
    "    return example\n",
    "\n",
    "# train_data = train_data.map(tokenize_msgs, num_proc=12)\n",
    "# val_data = val_data.map(tokenize_msgs, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.map(tokenize_msgs, num_proc=8)\n",
    "train_data = train_data.map(tokenize_msgs, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.encode('</commit_msg>')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.eos_token_id = eos_token_id\n",
    "model.decoder.config.eos_token_id = eos_token_id\n",
    "model.encoder.config.eos_token_id = eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(example):\n",
    "    example['labels'] = torch.squeeze(example['labels'])\n",
    "    example['input_ids'] = torch.squeeze(example['input_ids'])\n",
    "    example['attention_mask'] = torch.squeeze(example['attention_mask'])\n",
    "    return example\n",
    "\n",
    "val_data.set_format('torch')\n",
    "train_data.set_format('torch')\n",
    "\n",
    "train_data = train_data.map(squeeze, num_proc=8)\n",
    "val_data = val_data.map(squeeze, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, tokenizer, text, return_dict=False, seqs=5):\n",
    "    prompt = text\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding='max_length').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_outputs = model.generate(\n",
    "            **input,\n",
    "            max_new_tokens=200,\n",
    "            top_k=100,\n",
    "            num_return_sequences=seqs,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            early_stopping=True,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "    if not return_dict:\n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            print(\n",
    "                \"{}: {}\".format(\n",
    "                    i, tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "                )\n",
    "            )\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        res = []\n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            res.append(tokenizer.decode(sample_output, skip_special_tokens=False))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_prediction(model, tokenizer, data):\n",
    "    n = np.random.randint(0, len(data))\n",
    "    sample = data[n]\n",
    "    print(f\"Index - {n}\")\n",
    "    # pprint(f\"Language - {sample['language']}\")\n",
    "    print(f\"{'CODE_DIFSS':=^75}\")\n",
    "    print(sample[\"model_input\"])\n",
    "    print(f\"{'MESSAGE':=^75}\")\n",
    "    print(data[n][\"message\"], '\\n')\n",
    "    print(f\"{'GENERATED_MESSAGE':=^75}\")\n",
    "    generated = model_inference(model, tokenizer, sample[\"model_input\"], return_dict=True)\n",
    "    for elem in generated:\n",
    "        print(elem)\n",
    "        print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = transformers.GenerationConfig(\n",
    "            max_new_tokens=128,\n",
    "            top_k=100,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            early_stopping=True,\n",
    "            top_p=0.95,\n",
    "            bos_token_id=1,\n",
    "            decoder_start_token_id=0,\n",
    "            eos_token_id=eos_token_id,\n",
    "            pad_token_id=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bleu = evaluate.load('sacrebleu')\n",
    "\n",
    "\n",
    "train_args = transformers.Seq2SeqTrainingArguments(\n",
    "    f\"checkpoints_v3\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=10000,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=75000,\n",
    "    save_total_limit=4,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    report_to='wandb',\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_steps=100,\n",
    "    run_name='codeT5_V2_commit_msg_added',\n",
    "    generation_config=gen_config,\n",
    ")\n",
    "\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    try:\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    except Exception:\n",
    "        print(preds)\n",
    "        print(\"=\"*100)\n",
    "        print(labels)\n",
    "        raise ZeroDivisionError\n",
    "\n",
    "\n",
    "    result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang='en', device=device)\n",
    "    result = np.array(result[\"f1\"]).mean()\n",
    "    result = {\"BERTscore\": result}\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result['BLEU'] = bleu_score['score']\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = np.random.randint(0, len(val_data)-1, size=1500)\n",
    "val_samples = val_data.select(random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_samples,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikitasergeev692\u001b[0m (\u001b[33mnary\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/commits/commit_messages_generation/src/custom_CodeT5_CommitChronicle_V2/wandb/run-20231017_030019-5vajau47</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nary/commit_message_generation/runs/5vajau47' target=\"_blank\">codeT5_V2_commit_msg_added</a></strong> to <a href='https://wandb.ai/nary/commit_message_generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nary/commit_message_generation' target=\"_blank\">https://wandb.ai/nary/commit_message_generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nary/commit_message_generation/runs/5vajau47' target=\"_blank\">https://wandb.ai/nary/commit_message_generation/runs/5vajau47</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71710' max='239359' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 71710/239359 23:02:23 < 53:51:57, 0.86 it/s, Epoch 0.30/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bertscore</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.075200</td>\n",
       "      <td>2.743871</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>5.059000</td>\n",
       "      <td>42.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.831000</td>\n",
       "      <td>2.692919</td>\n",
       "      <td>0.872200</td>\n",
       "      <td>5.180600</td>\n",
       "      <td>40.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.757900</td>\n",
       "      <td>2.659077</td>\n",
       "      <td>0.873100</td>\n",
       "      <td>5.528100</td>\n",
       "      <td>36.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.717100</td>\n",
       "      <td>2.633897</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>3.969900</td>\n",
       "      <td>34.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>2.681300</td>\n",
       "      <td>2.620397</td>\n",
       "      <td>0.874200</td>\n",
       "      <td>4.804200</td>\n",
       "      <td>30.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>2.649400</td>\n",
       "      <td>2.612131</td>\n",
       "      <td>0.874200</td>\n",
       "      <td>4.714800</td>\n",
       "      <td>33.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>2.626400</td>\n",
       "      <td>2.602598</td>\n",
       "      <td>0.875700</td>\n",
       "      <td>4.805000</td>\n",
       "      <td>33.874700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/BERTscore</td><td>▁▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇██▇███</td></tr><tr><td>eval/BLEU</td><td>▆▆█▁▅▄▅▅▄▆▃▄▆▂▄▃▄▂▄▃▄▃▅</td></tr><tr><td>eval/gen_len</td><td>█▇▅▄▂▄▄▄▃▃▄▄▅▁▂▄▄▂▃▄▄▂▃</td></tr><tr><td>eval/loss</td><td>█▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▂▂▁▄▇▆▂▅▆▆▇█▄▅▅▇▅▆█▇▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▄▇▇█▄▂▂▆▃▂▂▂▁▄▃▃▁▃▂▁▂▂▃</td></tr><tr><td>eval/steps_per_second</td><td>▄▆▇█▄▂▃▆▃▃▃▂▁▄▄▃▂▃▃▂▂▂▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/BERTscore</td><td>0.8765</td></tr><tr><td>eval/BLEU</td><td>4.9595</td></tr><tr><td>eval/gen_len</td><td>31.4667</td></tr><tr><td>eval/loss</td><td>2.51152</td></tr><tr><td>eval/runtime</td><td>2035.3471</td></tr><tr><td>eval/samples_per_second</td><td>0.737</td></tr><tr><td>eval/steps_per_second</td><td>0.023</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>239359</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.4637</td></tr><tr><td>train/total_flos</td><td>4.664287386512916e+18</td></tr><tr><td>train/train_loss</td><td>2.58849</td></tr><tr><td>train/train_runtime</td><td>327548.0365</td></tr><tr><td>train/train_samples_per_second</td><td>23.384</td></tr><tr><td>train/train_steps_per_second</td><td>0.731</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">codeT5_V2_commit_msg_added</strong> at: <a href='https://wandb.ai/nary/commit_message_generation/runs/5vajau47' target=\"_blank\">https://wandb.ai/nary/commit_message_generation/runs/5vajau47</a><br/> View job at <a href='https://wandb.ai/nary/commit_message_generation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTczMTM2NA==/version_details/v4' target=\"_blank\">https://wandb.ai/nary/commit_message_generation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwNTczMTM2NA==/version_details/v4</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231017_030019-5vajau47/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commit-generation",
   "language": "python",
   "name": "commit-generation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
