{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e34d1f-7f64-4063-9f9d-68ca91dd9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "DIR_PREFIX = \"/home/user/commits/commit_messages_generation\"\n",
    "\n",
    "sys.path.insert(0, DIR_PREFIX)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\" #<- for the common server(SSH)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08cc7920-8e7b-4dee-83a5-ba193f8e730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "from peft import PeftModel, PeftConfig\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import wandb\n",
    "from transformers import TrainerCallback\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import PrefixTuningConfig\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import evaluate\n",
    "\n",
    "#import custom scripts\n",
    "from CommitChronicle_preprocessing.DatasetParser import DatasetParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246f8e36-beb5-4ad4-bda9-e432ac3555ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00573542e15d441591cec10c6a6f0fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(DIR_PREFIX + \"/CommitChronicle\")\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737b6f84-5758-4db0-b16c-ec56192e675b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"JetBrains-Research/cmg-codet5-without-history\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b60e7d94-fdf5-4116-aa3a-0d53dea2025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f703d837-0562-400d-aa37-9079fa0149db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input(example):\n",
    "    \"\"\"\n",
    "    function to parse code changes from CommitChronicle \n",
    "    It's adds special tokens to the sample(<code_del> , <code_add> ...)\n",
    "    Example of usage with the whole dataset:\n",
    "    >>> parser = DatasetParser(tokenizer)\n",
    "    >>> train_data = train_data.map(parser.parse_input, num_proc=8)\n",
    "    \"\"\"\n",
    "    diffs = []\n",
    "    for i in range(len(example[\"mods\"])):\n",
    "        change_type = example[\"mods\"][i][\"change_type\"]\n",
    "        new_path = (\n",
    "            example[\"mods\"][i][\"new_path\"] if example[\"mods\"][i][\"new_path\"] else \"\"\n",
    "        )\n",
    "        old_path =  (\n",
    "            example[\"mods\"][i][\"old_path\"] if example[\"mods\"][i][\"old_path\"] else \"\"\n",
    "        )\n",
    "\n",
    "        code_diff = example[\"mods\"][i][\"diff\"]\n",
    "        code_diff_lines = code_diff.split('\\n')\n",
    "        code_diff = '\\n'.join(code_diff_lines)\n",
    "        model_input = (old_path + \"\\n\" + new_path + \"\\n\" + code_diff + \"\\n\"\n",
    "        )\n",
    "        diffs.append(model_input)\n",
    "    example[\"model_input\"] = \"\\n\".join(diffs)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93259091-d4b5-422c-a740-f72b5e7a00c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba80e3bc0c946ebab25e3d36bf7d93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1554042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107426fff228420dac2c2ef5de614211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1554042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['message', 'language', 'model_input', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1554042\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = DatasetParser(tokenizer)\n",
    "val_data.set_format('torch')\n",
    "val_data = parser.remove_useless_columns(val_data)\n",
    "val_data = val_data.map(parser.tokenize_data, num_proc=10)\n",
    "val_data = val_data.map(parser.squeeze_dataset, num_proc=10)\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78ea7d72-a442-4cb8-be24-37b53733d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, tokenizer, text, return_dict=False, seqs=5):\n",
    "    prompt = text\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding='max_length').to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_outputs = model.generate(\n",
    "            **input,\n",
    "            max_new_tokens=25,\n",
    "            top_k=50,\n",
    "            num_return_sequences=seqs,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            early_stopping=True,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "    if not return_dict:\n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            print(\n",
    "                \"{}: {}\".format(\n",
    "                    i, tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "                )\n",
    "            )\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        res = []\n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            res.append(tokenizer.decode(sample_output, skip_special_tokens=True))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fa9cbff-133a-40c2-8edf-3b002db50b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_inference():\n",
    "    n = np.random.randint(0, len(val_data))\n",
    "    sample = val_data[n]\n",
    "    print(f\"Index - {n}\")\n",
    "    print(f\"{'CODE_DIFSS':=^75}\")\n",
    "    print(sample[\"model_input\"])\n",
    "    print(f\"{'MESSAGE':=^75}\")\n",
    "    print(val_data[n][\"message\"], '\\n')\n",
    "    print(f\"{'GENERATED_MESSAGE':=^75}\")\n",
    "    generated = model_inference(model, tokenizer, sample[\"model_input\"], return_dict=True)\n",
    "    for elem in generated:\n",
    "        print(elem)\n",
    "        print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d9c9116-5ea4-47ec-a3bd-df9255f2133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_eval = int(1e5)\n",
    "random_indeces = np.random.choice(len(val_data), size=samples_to_eval, replace=False)\n",
    "val_data = val_data.select(random_indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1436b5a4-8486-41f3-841f-8861e8cb0402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03469e88c24a4b808ff38c14559186ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "languages_used = np.array(val_data['language'])\n",
    "unique_langs = np.unique(languages_used)\n",
    "lang_datasets = {}\n",
    "for lang in unique_langs:\n",
    "    lang_datasets[lang] = []\n",
    "\n",
    "def distribute_samples(example):\n",
    "    global lang_datasets\n",
    "    lang = example['language']\n",
    "    lang_datasets[lang].append(example)\n",
    "    return example\n",
    "\n",
    "val_data = val_data.map(distribute_samples, num_proc=1, load_from_cache_file=False)\n",
    "for lang in lang_datasets.keys():\n",
    "    lang_datasets[lang] = datasets.Dataset.from_list(lang_datasets[lang])\n",
    "    lang_datasets[lang].set_format('torch')\n",
    "\n",
    "lang_datasets = datasets.DatasetDict(lang_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "249aea50-5d40-45be-86c5-a1cf757a6507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48733/2605824785.py:3: FutureWarning: Metric is deprecated and will be removed in the next major version of datasets. Use the new library 🤗 Evaluate instead: https://huggingface.co/docs/evaluate\n",
      "  bnorm = BLEUNorm()\n"
     ]
    }
   ],
   "source": [
    "from metrics.bnorm.bleu_norm import BLEUNorm\n",
    "\n",
    "bnorm = BLEUNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d160a45d-369e-4a5c-ad8d-3f1eb3662733",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_conf = transformers.GenerationConfig(\n",
    "    max_new_tokens=25,\n",
    "    top_k=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    top_p=0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01440f97-9629-4a71-8cea-b2d89807512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    #take only first 10 tokens to compute metric\n",
    "    preds = preds[:, :10]\n",
    "    labels = labels[:, :10]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = {}\n",
    "    result = bnorm.compute(predictions=decoded_preds, references=decoded_labels)['b_norm']\n",
    "    result = {\"BLEU_norm\": result}\n",
    "\n",
    "    prediction_lens = [torch.count_nonzero(pred != tokenizer.pad_token_id).item() for pred in preds]\n",
    "    result['prediction_len'] = np.array(prediction_lens).mean()\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84b1ed8c-7e27-45ae-8e6b-fc46f3419eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================C=====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53c422f6f024ca2b7fb2193c70980e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================C#=====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa41cedeeee4d888694c9164a3ceb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================C++====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad0cc1bba1a49d2b7b325b2f44dc349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Dart====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd22d008f07d4e17b07cc9c6105d356c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Elixir===================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bcc9c420104a7ab84495c44dc1ea24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================Go=====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def26b99fb704375b12d5cdb988ccdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Groovy===================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70da20aa9917473ca181c9aa377cf33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Java====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c894a840e14a3dba61763e7339e94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================JavaScript=================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8aad08c56c5424b9c2431bd69ec8370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Kotlin===================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e70c71948045e8baab4c195639858b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================Nix====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca49e91bf8c48758321405995b2b001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================Objective-C================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06cc4fef1114d35a671d8ec7ead13a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================PHP====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9778518c0afa46bea76c6665f1b42889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Python===================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457400a78c374180a0ee919679de73f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Ruby====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa90947a3351478985091ba0144ab4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Rust====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e461ae99e84d97aed7acbeb7dc10ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Shell===================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d20285d6bca40918a06443e1f2f40e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================Smalltalk=================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba03401e1f446c6b8bb3c1e838504ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Swift===================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6280d9d7de074a2dab1da6b933f3b096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================TypeScript=================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37db45c63ce44ef9abb0c8561abffc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_lang_results = {}\n",
    "total_BLEU = 0\n",
    "total_MSG_LEN = 0\n",
    "total_batches = 0\n",
    "for lang in lang_datasets.keys():\n",
    "    print(f\"{lang:=^75}\")\n",
    "    eval_dataloader = DataLoader(lang_datasets[lang], batch_size=32)\n",
    "    BLEU = 0\n",
    "    MSG_LEN = 0\n",
    "    progress = tqdm(enumerate(eval_dataloader), total=len(eval_dataloader))\n",
    "    for i, batch in progress:\n",
    "        batch_messages = batch[\"message\"]\n",
    "        batch_langs = batch[\"language\"]\n",
    "        batch_input_ids = batch[\"input_ids\"].to(device)\n",
    "        batch_attn = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels = batch[\"labels\"]\n",
    "\n",
    "        batch_encodings = {\n",
    "            \"input_ids\": batch_input_ids,\n",
    "            \"attention_mask\": batch_attn,\n",
    "            # \"decoder_input_ids\": batch_input_ids.clone(),\n",
    "        }\n",
    "        batch_preds = model.generate(\n",
    "            **batch_encodings, generation_config=generation_conf\n",
    "        )\n",
    "        eval_preds = (batch_preds, batch_labels)\n",
    "        batch_result = compute_metrics(eval_preds)\n",
    "        BLEU += batch_result[\"BLEU_norm\"]\n",
    "        total_BLEU += batch_result[\"BLEU_norm\"]\n",
    "        MSG_LEN += batch_result[\"prediction_len\"]\n",
    "        total_MSG_LEN += batch_result[\"prediction_len\"]\n",
    "        progress.set_description(\n",
    "            f\"BLEU: {BLEU / (i+1):.3f} LEN : {MSG_LEN / (i+1):.3f}\"\n",
    "        )\n",
    "    n = len(eval_dataloader)\n",
    "    total_batches += n\n",
    "    by_lang_results[lang] = {\n",
    "        \"BLEU_norm\": BLEU / len(eval_dataloader),\n",
    "        \"MSG_LEN\": MSG_LEN / len(eval_dataloader),\n",
    "    }\n",
    "\n",
    "by_lang_results['Total'] = {\n",
    "        \"BLEU_norm\": total_BLEU / total_batches,\n",
    "        \"MSG_LEN\": total_MSG_LEN / total_batches,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "998bd6ca-a074-42bf-86a1-98c470597169",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(by_lang_results).T\n",
    "metrics_df.to_csv('JB_BLEU_norm_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commit-generation",
   "language": "python",
   "name": "commit-generation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
